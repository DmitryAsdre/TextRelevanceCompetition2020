{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import dill\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, Lock, Value\n",
    "from IPython.display import clear_output\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import inscriptis\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import Stemmer\n",
    "import requests\n",
    "import subprocess\n",
    "from bs4.element import Comment\n",
    "import time\n",
    "\n",
    "import bokeh\n",
    "import bokeh.models\n",
    "import bokeh.plotting\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tf_sentencepiece  # Not used directly but needed to import TF ops.\n",
    "import sklearn.metrics.pairwise\n",
    "import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "DOC_NUM = 38114\n",
    "DATA = 'content/'\n",
    "DICT_FOLDER = 'dicts/'\n",
    "EXTRACTED = 'extracted/'\n",
    "EXTRACTED_RAW = 'extracted_raw/'\n",
    "EXTRACTED_PROCESSED = 'extracted_processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_rus = set(stopwords.words('russian'))\n",
    "stopwords_eng = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Загрузка***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_url_to_num():\n",
    "    url_to_num = dict()\n",
    "    num_to_url = dict()\n",
    "    with open('urls.numerate.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            #print(line)\n",
    "            line = line.strip('\\n')\n",
    "            line = line.split('\\t')\n",
    "            url_to_num[line[1]] = line[0]\n",
    "            num_to_url[line[0]] = line[1]\n",
    "    return url_to_num, num_to_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_num_to_path():\n",
    "    num_to_path = dict()\n",
    "    url_to_num , num_to_url = load_url_to_num()\n",
    "    counter = 0\n",
    "    for i in os.listdir(DATA):\n",
    "        for j in os.listdir(DATA + i):\n",
    "            path = DATA + i + '/' + j\n",
    "            with open(path, 'r', errors='replace') as f:\n",
    "                url = f.readline().strip()\n",
    "            num = url_to_num[url]\n",
    "            num_to_path[num] = path\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                clear_output(True)\n",
    "                print(counter)\n",
    "    return num_to_path, num_to_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38100\n"
     ]
    }
   ],
   "source": [
    "num_to_path, num_to_url = load_num_to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('num_to_path.dat', 'wb') as f:\n",
    "    dill.dump(num_to_path, f)\n",
    "with open('num_to_url.dat', 'wb') as f:\n",
    "    dill.dump(num_to_url, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('num_to_path.dat', 'rb') as f:\n",
    "    num_to_path = dill.load(f)\n",
    "with open('num_to_url.dat', 'rb') as f:\n",
    "    num_to_url = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Добавление документов*** <br>\n",
    "Часть документов испорчена. Я скачал их из архива за 2017 год и добавил.<br>\n",
    "Документы лежат в папке manually_downloaded и пронуменрованы doc_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38100\n"
     ]
    }
   ],
   "source": [
    "corrupted_nums = []\n",
    "for i in range(1, DOC_NUM + 1):\n",
    "    path = num_to_path[str(i)]\n",
    "    with open(path, 'r', errors='replace') as f:\n",
    "        doc = list(f)\n",
    "    body = ' '.join(doc[1:]).strip()\n",
    "    if body.lower() == 'aAqQsSwWdDeEfFrR'.lower():\n",
    "        corrupted_nums.append(i)\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in corrupted_nums:\n",
    "    url = num_to_url[str(num)]\n",
    "    path = num_to_path[str(num)]\n",
    "    try:\n",
    "        with open('manually_downloaded/' + str(num) + '.html', 'r', errors='replace') as f:\n",
    "            html_doc = f.read()\n",
    "        doc = '\\n'.join([url, html_doc])\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(doc)\n",
    "    except BaseException:\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Извлечение текста из файлов, обработка текста, лемматизация.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_0(text):\n",
    "    text = text.replace('-\\n', \"\")\n",
    "    #text = re.sub('(?: -|- |[^\\w-]+)', ' ', text) #удаляем все знаки препинания и т.п., но хотим оставить\n",
    "                                                  #слова вроде петропавловск-камчатский\n",
    "    text = re.sub(\"\\W+\", r' ', text)\n",
    "    text = ' '.join(re.split(\"(\\d+)\", text))\n",
    "    return text   \n",
    "def process_1(word):\n",
    "    word = word.replace(u'ё', u'е')\n",
    "    word = word.replace('_', '')\n",
    "    word = word.replace('_', '')\n",
    "    word = re.sub(' +', ' ', word)\n",
    "    word = word.strip()\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs4_text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    if not bool(soup.find()):\n",
    "        raise BaseException\n",
    "    \n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(num):\n",
    "    try:\n",
    "        stemmer_rus = Mystem()\n",
    "        stemmer_eng = Stemmer.Stemmer('english')\n",
    "        path = num_to_path[str(num)]\n",
    "        cur_doc = dict()\n",
    "        with open(path, 'r', errors='replace') as f:\n",
    "            doc = list(f)\n",
    "        url = num_to_url[str(num)]\n",
    "        doc = ''.join(doc[1:])\n",
    "        bs = BeautifulSoup(doc)\n",
    "        title = bs.find('title')\n",
    "        if title == None:\n",
    "            cur_doc['title'] = ''\n",
    "        else:\n",
    "            cur_doc['title'] = title.text \n",
    "        try:\n",
    "            text = bs4_text_from_html(doc)\n",
    "        except BaseException:\n",
    "            text = inscriptis.get_text(doc)\n",
    "        cur_doc['text'] = text\n",
    "        cur_doc['title'] = process_0(cur_doc['title']).lower()\n",
    "        cur_doc['title'] = ''.join(stemmer_rus.lemmatize(cur_doc['title'])).strip()\n",
    "        cur_doc['title'] = re.sub(\"\\W+\", r' ', cur_doc['title'])\n",
    "        cur_doc['title'] = ' ' .join(stemmer_eng.stemWords(cur_doc['title'].split(' ')))\n",
    "        cur_doc['title'] = ' '.join(list(map(process_1, nltk.tokenize.word_tokenize(cur_doc['title']))))\n",
    "        cur_doc['text'] = process_0(cur_doc['text']).lower()\n",
    "        cur_doc['text'] = ''.join(stemmer_rus.lemmatize(cur_doc['text'])).strip()\n",
    "        cur_doc['text'] = re.sub(\"\\W+\", r' ', cur_doc['text'])\n",
    "        cur_doc['text'] = ' '.join(stemmer_eng.stemWords(cur_doc['text'].split(' ')))\n",
    "        cur_doc['text'] = ' '.join(list(map(process_1, nltk.tokenize.word_tokenize(cur_doc['text']))))\n",
    "        with open(EXTRACTED + str(num) + '.dat', 'wb') as f:\n",
    "            dill.dump(cur_doc, f)\n",
    "    except BaseException:\n",
    "        with open('exceptions/' + str(num), 'w') as f:\n",
    "            f.write('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_text(num):\n",
    "    try:\n",
    "        path = num_to_path[str(num)]\n",
    "        with open(path, 'r', errors='replace') as f:\n",
    "            doc = list(f)\n",
    "        doc = ''.join(doc[1:])\n",
    "        bs = BeautifulSoup(doc)\n",
    "        title = bs.find('title')\n",
    "        if title == None:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = title.text\n",
    "        try:\n",
    "            text = bs4_text_from_html(doc)\n",
    "        except BaseException:\n",
    "            text = inscriptis.get_text(doc)\n",
    "        final_text = ' '.join([title, text])\n",
    "        #final_text = re.sub('\\n', ' ', final_text)\n",
    "        final_text = final_text.replace('-', ' ')\n",
    "        final_text = re.sub(' +', ' ', final_text)\n",
    "        with open(EXTRACTED_RAW + str(num) + '.dat', 'w', errors='replace') as f:\n",
    "            f.write(final_text)\n",
    "        final_text = process_0(final_text)\n",
    "        with open(EXTRACTED_PROCESSED + str(num) + '.dat', 'w', errors='replace') as f:\n",
    "            f.write(final_text)\n",
    "    except BaseException:\n",
    "        with open('exceptions/' + str(num) + '.dat', 'w') as f:\n",
    "            f.write('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_extract_raw(doc_id):\n",
    "    extract_raw_text(doc_id)\n",
    "    with mutex:\n",
    "        val.value += 1\n",
    "        if val.value % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(val.value, flush=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_extract(doc_id):\n",
    "    extract_text(doc_id)\n",
    "    with mutex:\n",
    "        val.value += 1\n",
    "        if val.value % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(val.value, flush=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Сбор статистики документов***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusStatistics(dict):\n",
    "    def __init__(self):\n",
    "        self['df'] = dict()\n",
    "        self['cf'] = dict()\n",
    "        self['count docs'] = 0\n",
    "        self['count lemms'] = 0\n",
    "        self['avgdl'] = 0\n",
    "    def idf(self, word):\n",
    "        df = 0\n",
    "        if word in self['df']:\n",
    "            df = self['df'][word]\n",
    "        if df == 0:\n",
    "            return 0\n",
    "        return self['count docs'] / df\n",
    "    def log_idf(self, word):\n",
    "        idf = self.idf(word)\n",
    "        if idf == 0:\n",
    "            return 0\n",
    "        return np.log(idf)\n",
    "    def icf(self, word):\n",
    "        if word in self['cf']:\n",
    "            cf = self['cf'][word]\n",
    "        else:\n",
    "            cf = 0\n",
    "        if cf == 0:\n",
    "            return 0\n",
    "        return self['count lemms'] / cf\n",
    "    def log_icf(self, word):\n",
    "        icf = self.icf(word)\n",
    "        if icf == 0:\n",
    "            return 0\n",
    "        return np.log(icf)\n",
    "        \n",
    "class Doc(dict):\n",
    "    def __init__(self):\n",
    "        self['text'] = dict()\n",
    "        self['text pos'] = dict()\n",
    "        self['title pos'] = dict()\n",
    "        self['title'] = dict()\n",
    "        self['len'] = 0\n",
    "        self['id'] = None\n",
    "    def count_word(self, word):\n",
    "        in_title = 0\n",
    "        in_text = 0\n",
    "        if word in self['title']:\n",
    "            in_title = self['title'][word]\n",
    "        if word in self['text']:\n",
    "            in_text = self['text'][word]\n",
    "        return in_title, in_text\n",
    "    def get_pos(self, word):\n",
    "        in_title = []\n",
    "        in_text = []\n",
    "        if word in self['title pos']:\n",
    "            in_title = self['title pos'][word]\n",
    "        if word in self['text pos']:\n",
    "            in_text = self['text pos'][word]\n",
    "        return in_title, in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(doc_id):\n",
    "    with open(DATA_FOLDER + str(doc_id) + '.dat', 'rb') as f:\n",
    "        cur_doc = dill.load(f)\n",
    "    doc = Doc()\n",
    "    #print(cur_doc['title'])\n",
    "    title = cur_doc['title']\n",
    "    text = cur_doc['text']\n",
    "    pos = 0\n",
    "    if title == None:\n",
    "        title = ''\n",
    "    for word in word_tokenize(title):\n",
    "        if word in doc['title']:\n",
    "            doc['title'][word] += 1\n",
    "        else:\n",
    "            doc['title'][word] = 1\n",
    "        \n",
    "        if word in doc['title pos']:\n",
    "            doc['title pos'][word].append(pos)\n",
    "        else:\n",
    "            doc['title pos'][word] = [pos]\n",
    "        pos += 1\n",
    "    for word in word_tokenize(text):\n",
    "        if word in doc['text']:\n",
    "            doc['text'][word] += 1\n",
    "        else:\n",
    "            doc['text'][word] = 1\n",
    "        \n",
    "        if word in doc['text pos']:\n",
    "            doc['text pos'][word].append(pos)\n",
    "        else:\n",
    "            doc['text pos'][word] = [pos]\n",
    "        pos += 1\n",
    "    doc['id'] = doc_id\n",
    "    doc['len'] = pos\n",
    "    with open(DICT_FOLDER + str(doc_id) + '.dat', 'wb') as f:\n",
    "        dill.dump(doc, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_info(doc_id):\n",
    "    extract_info(doc_id)\n",
    "    with mutex:\n",
    "        val.value += 1\n",
    "        if val.value % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(val.value, flush=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus_stats(dict_id):\n",
    "    with open(DICT_FOLDER + str(dict_id) + '.dat', 'rb') as f:\n",
    "        cur_doc = dill.load(f)\n",
    "    s = set(cur_doc['title'].keys())\n",
    "    s.update(cur_doc['text'].keys())\n",
    "    corpus_stats['count docs'] += 1\n",
    "    for word in s:\n",
    "        if word in corpus_stats['df']:\n",
    "            corpus_stats['df'][word] += 1\n",
    "        else:\n",
    "            corpus_stats['df'][word] = 1\n",
    "    for word in cur_doc['text']:\n",
    "        if word in corpus_stats['cf']:\n",
    "            corpus_stats['cf'][word] += cur_doc['text'][word]\n",
    "        else:\n",
    "            corpus_stats['cf'][word] = cur_doc['text'][word]\n",
    "    for word in cur_doc['title']:\n",
    "        if word in corpus_stats['cf']:\n",
    "            corpus_stats['cf'][word] += cur_doc['title'][word]\n",
    "        else:\n",
    "            corpus_stats['cf'][word] = cur_doc['title'][word]\n",
    "    corpus_stats['count lemms'] += cur_doc['len']        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DICT_FOLDER + 'corpus_stats.dat', 'rb') as f:\n",
    "    corpus_stats = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Вспомогательные функции***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sub(name):\n",
    "    sub = dict()\n",
    "    with open(name, 'r') as f:\n",
    "        submission = list(f)\n",
    "    for num, line in enumerate(submission):\n",
    "        if num == 0:\n",
    "            continue\n",
    "        line = line.split(',')\n",
    "        if int(line[0]) in sub:\n",
    "            sub[int(line[0])].append(int(line[1]))\n",
    "        else:\n",
    "            sub[int(line[0])] = [int(line[1])]\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queue():\n",
    "    stemmer = Mystem()\n",
    "    stemmer_eng = Stemmer.Stemmer('english')\n",
    "    orig_q = dict()\n",
    "    ext_q = dict()\n",
    "    with open('queue.txt', 'r') as f1:\n",
    "        with open('ext.txt', 'r') as f2:\n",
    "            for i in range(399):\n",
    "                line = f1.readline().strip()\n",
    "                line = line.split('\\t')\n",
    "                orig_q[int(line[0])] = line[1]\n",
    "                line = f2.readline()\n",
    "                line = line.split('\\t')\n",
    "                ext_q[int(line[0])] = line[1]\n",
    "            for i in orig_q:\n",
    "                orig_q[i] = process_0(orig_q[i])\n",
    "                ext_q[i] = process_0(ext_q[i])\n",
    "                orig_q[i] = ' '.join(list(filter(lambda x: not(x in stopwords_rus or x in stopwords_eng),nltk.tokenize.word_tokenize(orig_q[i]))))\n",
    "                ext_q[i] = ' '.join(list(filter(lambda x: not(x in stopwords_rus or x in stopwords_eng),nltk.tokenize.word_tokenize(ext_q[i]))))\n",
    "                orig_q[i] = ''.join(stemmer.lemmatize(orig_q[i]))\n",
    "                ext_q[i] = ''.join(stemmer.lemmatize(ext_q[i]))\n",
    "                orig_q[i] = ' '.join(stemmer_eng.stemWords(orig_q[i].split(' ')))\n",
    "                ext_q[i] = ' '.join(stemmer_eng.stemWords(ext_q[i].split(' ')))\n",
    "                orig_q[i] = ' '.join(list(map(process_1, nltk.tokenize.word_tokenize(orig_q[i]))))\n",
    "                ext_q[i] = ' '.join(list(map(process_1, nltk.tokenize.word_tokenize(ext_q[i]))))\n",
    "    return orig_q, ext_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***bm-25***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ score(D,Q) = \\sum_{i=0}^n IDF(q_i) \\frac{f(q_i,D)*(k_1 + 1)}{f(q_i,D) + k_1*(1 - b + b \\frac{|D|}{avgdl})} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$IDF(q_i) = log{\\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_score(q_i, doc, corpus_statistics):\n",
    "    in_title, in_text = doc.count_word(q_i)\n",
    "    tf = (in_title + in_text)/doc['len']\n",
    "    log_idf = corpus_statistics.log_idf(q_i)\n",
    "    avgdl = corpus_statistics['avgdl']\n",
    "    return log_idf * tf*(k1 + 1) / (tf + k1 * (1 - b + b*doc['len']/avgdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(q, doc, corpus_statistics):\n",
    "    score = 0\n",
    "    if doc['len'] == 0:\n",
    "        return 0\n",
    "    for q_i in nltk.tokenize.word_tokenize(q):\n",
    "        score += single_score(q_i, doc, corpus_statistics)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_bm25(clean_q, clean_e, corpus_statistics, queue_to_nums):\n",
    "    final_res = dict()\n",
    "    counter = 0\n",
    "    for queue_id in range(1, len(clean_q) + 1):\n",
    "        nums_for_queue = queue_to_nums[queue_id]\n",
    "        final_res[queue_id] = dict()\n",
    "        for doc_id in nums_for_queue:\n",
    "            score = 0\n",
    "            with open(DICT_FOLDER + str(doc_id) + '.dat', 'rb') as f:\n",
    "                doc = dill.load(f)\n",
    "            q_i = clean_q[queue_id]#list(nltk.tokenize.word_tokenize(clean_q[queue_id]))\n",
    "            e_i = clean_e[queue_id]#list(nltk.tokenize.word_tokenize(clean_e[queue_id]))\n",
    "            #print(doc['len'], doc_id)\n",
    "            score = bm25(q_i, doc, corpus_statistics)\n",
    "            final_res[queue_id][doc_id] = score\n",
    "        clear_output(True)\n",
    "        print(queue_id)\n",
    "    sub = dict()\n",
    "    for queue_id in range(1, len(clean_q) + 1):\n",
    "        idx = []\n",
    "        values = []\n",
    "        for i in final_res[queue_id]:\n",
    "            values.append(final_res[queue_id][i])\n",
    "            idx.append(i)\n",
    "        values = np.array(values)\n",
    "        idx = np.array(idx)\n",
    "        idx = idx[np.argsort(values)[::-1][:10]]\n",
    "        sub[queue_id] = idx\n",
    "    return sub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***bm-25 вторая формула***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$score(D,Q) = \\sum_{i=0}^n IDF(q_i) \\frac{f(q_i,D)*(k_1 + 1)}{f(q_i,D) + k_1*(1 - b + b \\frac{|D|}{avgdl})}*\\frac{(k_2 + 1)*qf_i}{k_2 + qf_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_score_2 (q_i, doc, corpus_statistics):\n",
    "    in_title, in_text = doc.count_word(q_i)\n",
    "    tf = (in_title + in_text)/doc['len']\n",
    "    log_idf = corpus_statistics.log_idf(q_i)\n",
    "    avgdl = corpus_statistics['avgdl']\n",
    "    return log_idf * tf*(k1 + 1) / (tf + k1 * (1 - b + b*doc['len']/avgdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_2(q, doc, corpus_statistics):\n",
    "    score = 0\n",
    "    if doc['len'] == 0:\n",
    "        return 0\n",
    "    freqs = get_freq(q)\n",
    "    for q_i in nltk.tokenize.word_tokenize(q):\n",
    "        score += single_score(q_i, doc, corpus_statistics) * (k2 + 1) * freqs[q_i]/(k2 + freqs[q_i])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(q):\n",
    "    freqs = {}\n",
    "    for q_i in nltk.word_tokenize(q):\n",
    "        if q_i in freqs:\n",
    "            freqs[q_i] += 1\n",
    "        else:\n",
    "            freqs[q_i] = 1\n",
    "    for q_i in q.split(' '):\n",
    "        freqs[q_i] /= len(list(nltk.word_tokenize(q)))\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_bm25(clean_q, clean_e, corpus_statistics, queue_to_nums):\n",
    "    final_res = dict()\n",
    "    counter = 0\n",
    "    for queue_id in range(1, len(clean_q) + 1):\n",
    "        nums_for_queue = queue_to_nums[queue_id]\n",
    "        final_res[queue_id] = dict()\n",
    "        for doc_id in nums_for_queue:\n",
    "            score = 0\n",
    "            with open(DICT_FOLDER + str(doc_id) + '.dat', 'rb') as f:\n",
    "                doc = dill.load(f)\n",
    "            q_i = clean_q[queue_id]#list(nltk.tokenize.word_tokenize(clean_q[queue_id]))\n",
    "            e_i = clean_e[queue_id]#list(nltk.tokenize.word_tokenize(clean_e[queue_id]))\n",
    "            #print(doc['len'], doc_id)\n",
    "            score = bm25(q_i, doc, corpus_statistics)\n",
    "            final_res[queue_id][doc_id] = score\n",
    "        clear_output(True)\n",
    "        print(queue_id)\n",
    "    sub = dict()\n",
    "    for queue_id in range(1, len(clean_q) + 1):\n",
    "        idx = []\n",
    "        values = []\n",
    "        for i in final_res[queue_id]:\n",
    "            values.append(final_res[queue_id][i])\n",
    "            idx.append(i)\n",
    "        values = np.array(values)\n",
    "        idx = np.array(idx)\n",
    "        idx = idx[np.argsort(values)[::-1][:10]]\n",
    "        sub[queue_id] = idx\n",
    "    return sub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Формула из статьи 'Алгоритм текстового ранжирования Яндекса на РОМИП-2006'***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Score = W_{single} + W_{pair} + k_1*W_{AllWords} + k_2*W_{Phrase} + k_3*W_{HalfPhrase} + W_{PRF}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W_{single} = log{p}*(TF_1 + s*TF_2), s = 0.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ TF_1 = \\frac{TF}{TF + k_1 + k_2*DocLength}, k_1 =1, k_2 = \\frac{1}{350}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ TF_2 = \\frac{Hdr}{1 + Hdr} $$\n",
    "$$ TF_2 = W_{title} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***В качестве p я брал ICF и IDF***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p = ICF $$ <br>\n",
    "$$ p = IDF $$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Учет пар слов***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова из запроса встречаются в тексте подряд - +1<br>\n",
    "Слова из запроса встречаются в тексте через одно слова - +0.5<br>\n",
    "Слова из запроса встречаются в тексте в обратном порядке - +0.5<br>\n",
    "Слова из запроса, которые идут через слово, встречаются в тексте подряд - 0.1<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W_{pair} = 0.3*(log{p_1} + log{p_2}) * \\frac{TF}{1 + TF} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p = ICF $$ <br>\n",
    "$$ p = IDF $$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Учет всех слов***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W_{AllWords} = 0.2 * \\sum log(p_i) * 0.03^{N_{miss}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p = ICF $$ <br>\n",
    "$$ p = IDF $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W_{phrase} = 0.1 * \\sum log(p_i)*\\frac{TF}{1 + TF} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Учет половины фразы***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W_{halfphrase} = 0.02 * \\sum log(p_i) *\\frac{TF}{1 + TF} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_word(word, corpus_stats, doc):\n",
    "    in_title, in_text= doc.count_word(word)\n",
    "    tf = in_title + in_text\n",
    "    score1 = tf / (tf + 1.0 + 1.0/1000.0 * doc['len'])\n",
    "    hdr = in_title * 3.0 + in_text\n",
    "    score2 = int(in_title > 0)#hdr / (1.0 + hdr)\n",
    "    return corpus_stats.log_idf(word) * (score1 + 1.0 * score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_count(w1, w2, pos1, pos2):\n",
    "    count_row = 0\n",
    "    count_inverse = 0\n",
    "    count_skip = 0\n",
    "    for p1 in pos1:\n",
    "        if (p1 + 1) in pos2:\n",
    "            count_row += 1\n",
    "        if (p1 - 1) in pos2:\n",
    "            count_inverse += 1\n",
    "        if (p1 + 2) in pos2:\n",
    "            count_skip += 1\n",
    "    return count_row, count_inverse, count_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_miss(words, doc, th, corpus_stats):\n",
    "    count_miss = 0\n",
    "    for w in words:\n",
    "        w_idf = corpus_stats.log_idf(w)\n",
    "        in_title, in_text = doc.count_word(w)\n",
    "        all_count = in_title + in_text\n",
    "        if all_count == 0:\n",
    "            if w_idf > th:\n",
    "                count_miss += 1\n",
    "            continue\n",
    "    return count_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_words(words, doc, corpus_stats):\n",
    "    score = 0\n",
    "    for w in words:\n",
    "        score += single_word(w, corpus_stats, doc)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_include_all_words(words, doc, th, corpus_stats):\n",
    "    score = 0\n",
    "    for w in words:\n",
    "        score += corpus_stats.log_idf(w)\n",
    "    score *= 0.03 ** (count_miss(words, doc, th, corpus_stats))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_query_pairs(words, doc, corpus_stats):\n",
    "    score = 0\n",
    "    for i in range(len(words) - 1):\n",
    "        w1 = words[i]\n",
    "        w2 = words[i + 1]\n",
    "        in_title1, in_text1 = doc.get_pos(w1)\n",
    "        pos1 = set(in_title1)\n",
    "        pos1.update(in_text1)\n",
    "        in_title2, in_text2 = doc.get_pos(w2)\n",
    "        pos2 = set(in_title2)\n",
    "        pos2.update(in_text2)\n",
    "        count_row, count_inverse, count_skip = pair_count(w1, w2, pos1, pos2)\n",
    "        pair_tf = count_row * 1.5 + count_inverse * 0.7 + count_skip*0.5\n",
    "        score += (corpus_stats.log_idf(w1) + corpus_stats.log_idf(w2))*pair_tf/(1 + pair_tf)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases(half_phrase, phrase, doc_num, corpus_stats):\n",
    "    with open(EXTRACTED + str(doc_num) + '.dat', 'rb') as f:\n",
    "        cur_doc = dill.load(f)\n",
    "    tf_title = cur_doc['title'].count(phrase)\n",
    "    tf_body = cur_doc['text'].count(phrase)\n",
    "    tf_phrase = tf_body + 2.0 * tf_title\n",
    "    tf_title = doc['title'].count(half_phrase)\n",
    "    tf_body = cur_doc['text'].count(half_phrase)\n",
    "    tf_half_phrase = tf_body + 2.0 * tf_title\n",
    "    log_idf_hp = 0\n",
    "    for q_i in nltk.word_tokenize(half_phrase):\n",
    "        log_idf_hp += corpus_stats.log_idf(q_i)\n",
    "    log_idf_p = 0\n",
    "    for q_i in nltk.word_tokenize(phrase):\n",
    "        log_idf_p += corpus_stats.log_idf(q_i)\n",
    "    W_p = log_idf_p * (tf_phrase / ( 1 + tf_phrase))\n",
    "    W_hp = log_idf_hp * (tf_half_phrase / (1 + tf_half_phrase))\n",
    "    return 0.2 * W_p + 0.01 * W_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_romip(clean_q, clean_e, corpus_stats, queue_to_nums):\n",
    "    final_res = dict()\n",
    "    counter = 0\n",
    "    for queue_id in range(1, len(clean_q) + 1):\n",
    "        nums_for_queue = queue_to_nums[queue_id]\n",
    "        final_res[queue_id] = dict()\n",
    "        for doc_id in nums_for_queue:\n",
    "            score = 0\n",
    "            with open(DICT_FOLDER + str(doc_id) + '.dat', 'rb') as f:\n",
    "                doc = dill.load(f)\n",
    "            q_i = list(nltk.tokenize.word_tokenize(clean_q[queue_id]))\n",
    "            e_i = list(nltk.tokenize.word_tokenize(clean_e[queue_id]))\n",
    "            score += 0.3 * score_words(e_i, doc, corpus_stats)\n",
    "            score += 0.3 * 0.3 * score_query_pairs(e_i, doc, corpus_stats)\n",
    "            score += score_words(q_i, doc, corpus_stats)\n",
    "            score += 0.3 * score_query_pairs(q_i, doc, corpus_stats)\n",
    "            #score += score_phrases(q_i, '', doc_id, corpus_stats)\n",
    "            idf_array = []\n",
    "            for w in q_i:\n",
    "                idf_array.append(corpus_stats.log_idf(w))\n",
    "            med_idf = np.median(idf_array)/2\n",
    "            score += 0.2 * score_include_all_words(q_i, doc, med_idf, corpus_stats)\n",
    "            final_res[queue_id][doc_id] = score\n",
    "        clear_output(True)\n",
    "        print(queue_id)\n",
    "    sub = dict()\n",
    "    vals = dict()\n",
    "    for queue_id in range(1, len(clean_q) + 1):\n",
    "        idx = []\n",
    "        values = []\n",
    "        for i in final_res[queue_id]:\n",
    "            idx.append(i)\n",
    "            values.append(final_res[queue_id][i])\n",
    "        values = np.array(values)\n",
    "        idx = np.array(idx)\n",
    "        idx = idx[np.argsort(values)[::-1][:15]]\n",
    "        vals[queue_id] = values[np.argsort(values)[::-1][:15]]\n",
    "        sub[queue_id] = idx\n",
    "    return sub, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_q, clean_e = load_queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_for_queue = dict()\n",
    "with open('sample.technosphere.ir1.textrelevance.submission.txt', 'r') as f:\n",
    "    lines = list(f)\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(',')\n",
    "    if line[0] == 'QueryId':\n",
    "        continue\n",
    "    q_id = int(line[0])\n",
    "    doc_id = int(line[1])\n",
    "    if q_id in num_for_queue:\n",
    "        num_for_queue[q_id].append(doc_id)\n",
    "    else:\n",
    "        num_for_queue[q_id] = [doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n"
     ]
    }
   ],
   "source": [
    "sub, vals = scoring_romip(clean_q, clean_e, corpus_stats, num_for_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Embeddings***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universal Sentence Encoder Multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dmitry/Applications/anaconda3/envs/kaggle_comp/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/1'  #@param ['https://tfhub.dev/google/universal-sentence-encoder-multilingual/1', 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1', 'https://tfhub.dev/google/universal-sentence-encoder-xling-many/1']\n",
    "\n",
    "# Set up graph.\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "  multiling_embed = hub.Module(module_url)\n",
    "  embedded_text = multiling_embed(text_input)\n",
    "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "# Initialize session.\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n"
     ]
    }
   ],
   "source": [
    "res_sub = dict()\n",
    "res_vals = dict()\n",
    "for j in sub:\n",
    "    cur_top = np.array(sub[j])\n",
    "    cur_queue = clean_q[j]\n",
    "    corpus = [cur_queue]\n",
    "    for doc_id in cur_top:\n",
    "        with open(EXTRACTED_RAW + str(doc_id) + '.dat', 'r') as f:\n",
    "            doc = f.read()\n",
    "        if len(doc) > 700_000:\n",
    "            doc = doc[:700_000]\n",
    "        corpus.append(doc)\n",
    "    embeddings_res = session.run(embedded_text, feed_dict={text_input : corpus})\n",
    "    cosine_dists = sklearn.metrics.pairwise.cosine_similarity(embeddings_res)[0,1:]\n",
    "    res_vals[j] = cosine_dists\n",
    "    with open('checkpoints/' + str(j) + '.dat', 'wb') as f:\n",
    "        dill.dump(sub, f)\n",
    "    clear_output(True)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве финального ранжирования:<br>\n",
    "Используя алгоритм РОМИП получаем топ 15 текстов и score для каждого из них. Для этих текстов вычисляем эмбединги. Используя их, вычисляем cosine_similarity. Получаем два вектора $$score_{bm25}$$ и $$score_{nn}$$.\n",
    "Окончательно ранжируем - $$ score_{final} = 0.8 \\frac{score_{bm25}}{|score_{bm25}|} + 0.2\\frac{score_{nn}}{|score_{nn}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_vals.dump', 'wb') as f:\n",
    "    dill.dump(res_vals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_vals.dump', 'rb') as f:\n",
    "    res_vals = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub = dict()\n",
    "for s in sub:\n",
    "    doc_ids = np.array(sub[s])\n",
    "    nn_weights = np.array(res_vals[s])\n",
    "    bm_weights = np.array(vals[s])\n",
    "    #nn_weights = np.abs(nn_weights)\n",
    "    nn_weights /= sum(nn_weights)\n",
    "    bm_weights /= sum(bm_weights)\n",
    "    final_weights = 0.8 * bm_weights + 0.2 * nn_weights\n",
    "    final_sub[s] = doc_ids[np.argsort(final_weights)[::-1]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sub(name, sub):\n",
    "    with open(name + '.sub', 'w') as f:\n",
    "        f.write('QueryId,DocumentId' + '\\n')\n",
    "        for q_id in sub:\n",
    "            for res in sub[q_id]:\n",
    "                f.write(str(q_id) + ',' + str(res) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sub('final_sub', final_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Финальное решение***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutex = Lock()\n",
    "val = Value('i', 0)\n",
    "with Pool(processes=6) as pool1:\n",
    "    pool1.map(wrapper_extract_raw, [i for i in range(1, DOC_NUM + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutex = Lock()\n",
    "val = Value('i', 0)\n",
    "with Pool(processes=6) as pool1:\n",
    "    pool1.map(wrapper_extract, [i for i in range(1, DOC_NUM + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38100\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, DOC_NUM + 1):\n",
    "    extract_corpus_stats(i)\n",
    "    #clear_output(True)\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_stats['avgdl'] = corpus_stats['count lemms'] / corpus_stats['count docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_q, clean_e = load_queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_for_queue = dict()\n",
    "with open('sample.technosphere.ir1.textrelevance.submission.txt', 'r') as f:\n",
    "    lines = list(f)\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(',')\n",
    "    if line[0] == 'QueryId':\n",
    "        continue\n",
    "    q_id = int(line[0])\n",
    "    doc_id = int(line[1])\n",
    "    if q_id in num_for_queue:\n",
    "        num_for_queue[q_id].append(doc_id)\n",
    "    else:\n",
    "        num_for_queue[q_id] = [doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n"
     ]
    }
   ],
   "source": [
    "sub, vals = scoring_romip(clean_q, clean_e, corpus_stats, num_for_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_sub = dict()\n",
    "res_vals = dict()\n",
    "for j in sub:\n",
    "    cur_top = np.array(sub[j])\n",
    "    cur_queue = clean_q[j]\n",
    "    corpus = [cur_queue]\n",
    "    for doc_id in cur_top:\n",
    "        with open(EXTRACTED_RAW + str(doc_id) + '.dat', 'r') as f:\n",
    "            doc = f.read()\n",
    "        if len(doc) > 700_000:\n",
    "            doc = doc[:700_000]\n",
    "        corpus.append(doc)\n",
    "    embeddings_res = session.run(embedded_text, feed_dict={text_input : corpus})\n",
    "    cosine_dists = sklearn.metrics.pairwise.cosine_similarity(embeddings_res)[0,1:]\n",
    "    res_vals[j] = cosine_dists\n",
    "    with open('checkpoints/' + str(j) + '.dat', 'wb') as f:\n",
    "        dill.dump(sub, f)\n",
    "    clear_output(True)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub = dict()\n",
    "for s in sub:\n",
    "    doc_ids = np.array(sub[s])\n",
    "    nn_weights = np.array(res_vals[s])\n",
    "    bm_weights = np.array(vals[s])\n",
    "    #nn_weights = np.abs(nn_weights)\n",
    "    nn_weights /= sum(nn_weights)\n",
    "    bm_weights /= sum(bm_weights)\n",
    "    final_weights = 0.8 * bm_weights + 0.2 * nn_weights\n",
    "    final_sub[s] = doc_ids[np.argsort(final_weights)[::-1]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sub('final_sub', final_sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
